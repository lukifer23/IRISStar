package com.nervesparks.iris.viewmodel

import android.llama.cpp.LLamaAndroid
import android.util.Log
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.setValue
import androidx.lifecycle.ViewModel
import androidx.lifecycle.viewModelScope
import com.nervesparks.iris.data.UserPreferencesRepository
import dagger.hilt.android.lifecycle.HiltViewModel
import kotlinx.coroutines.delay
import kotlinx.coroutines.launch
import kotlinx.coroutines.withTimeout
import java.io.File
import javax.inject.Inject
import kotlin.time.Duration.Companion.seconds

/**
 * PHASE 1.6: BenchmarkViewModel - Extracted from MainViewModel
 * Handles performance testing, benchmarking, and metrics collection
 */
@HiltViewModel
class BenchmarkViewModel @Inject constructor(
    private val llamaAndroid: LLamaAndroid,
    private val userPreferencesRepository: UserPreferencesRepository
) : ViewModel() {

    private val tag = "BenchmarkViewModel"

    // Benchmark state
    var isBenchmarkRunning by mutableStateOf(false)
    var benchmarkProgress by mutableStateOf(0f)
    var benchmarkResults by mutableStateOf<Map<String, Any>>(emptyMap())
    var benchmarkError by mutableStateOf<String?>(null)

    // Model selection for benchmark
    var showModelSelection by mutableStateOf(false)
    var selectedModelForBenchmark by mutableStateOf("")
    var availableModelsForBenchmark by mutableStateOf<List<Map<String, String>>>(emptyList())

    // Performance metrics
    var tokensPerSecond by mutableStateOf(0f)
    var timeToFirstToken by mutableStateOf(0L)
    var totalTokensGenerated by mutableStateOf(0)
    var memoryUsage by mutableStateOf(0L)
    var contextLimit by mutableStateOf(0)

    // Benchmark configuration
    private val benchmarkPrompt = "Write a comprehensive article about artificial intelligence, covering its history, current applications, and future potential. Make it detailed and informative."
    private val benchmarkTimeout = 120.seconds

    // Standard benchmark function
    fun runStandardBenchmark() {
        viewModelScope.launch {
            try {
                isBenchmarkRunning = true
                benchmarkProgress = 0f
                benchmarkError = null
                resetMetrics()

                Log.d(tag, "Starting standard benchmark")

                // Phase 1: Warm-up (20%)
                benchmarkProgress = 0.2f
                delay(1000)

                // Phase 2: Token generation test (60%)
                val results = runTokenGenerationTest()
                benchmarkProgress = 0.8f

                // Phase 3: Memory and performance analysis (20%)
                analyzePerformance()
                benchmarkProgress = 1.0f

                benchmarkResults = results
                Log.d(tag, "Benchmark completed: $results")

            } catch (e: Exception) {
                Log.e(tag, "Benchmark failed", e)
                benchmarkError = "Benchmark failed: ${e.message}"
            } finally {
                isBenchmarkRunning = false
            }
        }
    }

    private suspend fun runTokenGenerationTest(): Map<String, Any> {
        val startTime = System.currentTimeMillis()
        var tokensGenerated = 0
        var firstTokenTime = 0L

        try {
            withTimeout(benchmarkTimeout) {
                llamaAndroid.send(benchmarkPrompt)
                    .collect { token ->
                        tokensGenerated++
                        if (firstTokenTime == 0L) {
                            firstTokenTime = System.currentTimeMillis() - startTime
                        }

                        // Update progress based on tokens
                        benchmarkProgress = 0.2f + (tokensGenerated.toFloat() / 100f).coerceAtMost(0.6f)
                    }
            }
        } catch (e: Exception) {
            Log.w(tag, "Token generation test completed or timed out", e)
        }

        val endTime = System.currentTimeMillis()
        val totalTime = endTime - startTime

        return mapOf(
            "totalTokens" to tokensGenerated,
            "totalTimeMs" to totalTime,
            "tokensPerSecond" to if (totalTime > 0) (tokensGenerated * 1000f / totalTime) else 0f,
            "timeToFirstTokenMs" to firstTokenTime,
            "benchmarkPrompt" to benchmarkPrompt.take(100) + "..."
        )
    }

    private suspend fun analyzePerformance() {
        try {
            // Get memory usage
            memoryUsage = llamaAndroid.getMemoryUsage()
            Log.d(tag, "Memory usage: $memoryUsage bytes")

            // Get offload information
            val offloadCounts = llamaAndroid.getOffloadCounts()
            Log.d(tag, "GPU offload: $offloadCounts")

        } catch (e: Exception) {
            Log.w(tag, "Performance analysis failed", e)
        }
    }

    // Comparative benchmark
    fun runComparativeBenchmark() {
        viewModelScope.launch {
            try {
                isBenchmarkRunning = true
                benchmarkProgress = 0f
                benchmarkError = null

                Log.d(tag, "Starting comparative benchmark")

                // Test different configurations
                val configs = listOf(
                    "CPU" to "cpu",
                    "GPU OpenCL" to "opencl",
                    "GPU Vulkan" to "vulkan"
                )

                val results = mutableMapOf<String, Map<String, Any>>()

                for ((name, backend) in configs) {
                    try {
                        // Switch backend
                        val success = llamaAndroid.setBackend(backend)
                        if (success) {
                            Log.d(tag, "Testing backend: $backend")
                            val result = runTokenGenerationTest()
                            results[name] = result
                        } else {
                            Log.w(tag, "Backend $backend not available")
                        }
                    } catch (e: Exception) {
                        Log.w(tag, "Failed to test backend: $backend", e)
                    }

                    benchmarkProgress += 1f / configs.size
                }

                benchmarkResults = mapOf("comparative" to results)
                Log.d(tag, "Comparative benchmark completed")

            } catch (e: Exception) {
                Log.e(tag, "Comparative benchmark failed", e)
                benchmarkError = "Comparative benchmark failed: ${e.message}"
            } finally {
                isBenchmarkRunning = false
            }
        }
    }

    // Model-specific benchmark
    fun runBenchmarkWithModel(modelName: String, directory: File) {
        viewModelScope.launch {
            try {
                isBenchmarkRunning = true
                benchmarkProgress = 0f
                benchmarkError = null

                Log.d(tag, "Running benchmark with model: $modelName")

                // Load the model if not already loaded
                val modelPath = File(directory, modelName).absolutePath
                llamaAndroid.load(modelPath, 4, 40, 0.9f, 0.7f, -1)

                // Run benchmark
                val results = runTokenGenerationTest()

                benchmarkResults = mapOf(
                    "model" to modelName,
                    "results" to results
                )

                Log.d(tag, "Model benchmark completed for: $modelName")

            } catch (e: Exception) {
                Log.e(tag, "Model benchmark failed", e)
                benchmarkError = "Model benchmark failed: ${e.message}"
            } finally {
                isBenchmarkRunning = false
            }
        }
    }

    // Performance monitoring
    fun startPerformanceMonitoring() {
        viewModelScope.launch {
            while (true) {
                try {
                    updatePerformanceMetrics()
                    delay(1000) // Update every second
                } catch (e: Exception) {
                    Log.w(tag, "Performance monitoring error", e)
                }
            }
        }
    }

    private suspend fun updatePerformanceMetrics() {
        try {
            memoryUsage = llamaAndroid.getMemoryUsage()
            contextLimit = llamaAndroid.countTokens(benchmarkPrompt)
        } catch (e: Exception) {
            Log.w(tag, "Failed to update performance metrics", e)
        }
    }

    // Utility functions
    private fun resetMetrics() {
        tokensPerSecond = 0f
        timeToFirstToken = 0L
        totalTokensGenerated = 0
        memoryUsage = 0L
        contextLimit = 0
    }

    fun showBenchmarkModelSelection() {
        showModelSelection = true
        Log.d(tag, "Showing model selection for benchmark")
    }

    fun hideBenchmarkModelSelection() {
        showModelSelection = false
        selectedModelForBenchmark = ""
        Log.d(tag, "Hiding model selection")
    }

    fun selectModelForBenchmark(modelName: String) {
        selectedModelForBenchmark = modelName
        Log.d(tag, "Selected model for benchmark: $modelName")
    }

    fun updateAvailableModelsForBenchmark(models: List<Map<String, String>>) {
        availableModelsForBenchmark = models
        Log.d(tag, "Set ${models.size} models available for benchmark")
    }

    fun resetBenchmark() {
        benchmarkResults = emptyMap()
        benchmarkError = null
        benchmarkProgress = 0f
        resetMetrics()
        Log.d(tag, "Benchmark reset")
    }

    // Get benchmark summary
    fun getBenchmarkSummary(): String {
        return when {
            benchmarkError != null -> "Error: $benchmarkError"
            benchmarkResults.isNotEmpty() -> {
                val tokensPerSec = benchmarkResults["tokensPerSecond"] as? Float ?: 0f
                val totalTokens = benchmarkResults["totalTokens"] as? Int ?: 0
                "Generated $totalTokens tokens at ${String.format("%.1f", tokensPerSec)} TPS"
            }
            else -> "No benchmark results available"
        }
    }
}
